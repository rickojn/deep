rows: 28, cols: 28
Number of training images: 60000
rows: 28, cols: 28
Number of test images: 10000
Model created with 2 layers
Layer 0: 784 inputs, 8 neurons
Layer 1: 8 inputs, 10 neurons
Number of parameters: 6370
Batch size: 16

No model files found in directory /home/rickojn/coding/deep/models/

No model found, training from scratch
matmul simd ....
Time spent in matmul_simd_forward: 0.035459 seconds
relu forward ...
Time spent in relu_forward: 0.000252 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000791 seconds
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
softmax forward ...
Time spent in softmax_forward: 0.001020 seconds
Test loss before training: 2.335670
Test accuracy before training: 0.097500


training loop:

epoch: 0
matmul simd ....
Time spent in matmul_simd_forward: 0.000031 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000003 seconds
logit = 0.126389
logit = -0.190522
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
 input 0.000000   z grad 0.108971  prob 0.108971
 input 0.992157   z grad 0.072773  prob 0.072773
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad -0.900000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad -0.900000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
matmul simd backward weight grads ...
input = 0.000000 z grad = 0.108971  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.992157 z grad = 0.072773  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = -0.900000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = -0.900000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
Time spent in simd_matmul_backward weight grads: 0.000012 seconds
matmul simd backward input grads ...
Time spent for input activation grads: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004513
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul simd backward weight grads ...
Time spent in simd_matmul_backward weight grads: 0.000016 seconds
matmul simd backward input grads ...
Time spent for input activation grads: 0.000070 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.291618
Training accuracy: 0.125000

epoch: 1
matmul simd ....
Time spent in matmul_simd_forward: 0.000045 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000002 seconds
logit = 0.304883
logit = 0.401266
logit = 0.002614
logit = 0.002614
logit = 0.002614
logit = 0.002614
logit = 0.002614
logit = 0.002614
logit = 0.002614
logit = 0.002614
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
 input 0.000000   z grad 0.118916  prob 0.118916
 input 0.000000   z grad 0.125383  prob 0.125383
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad -0.899740  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad -0.899740  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad -0.899740  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
matmul simd backward weight grads ...
input = 0.000000 z grad = 0.118916  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.125383  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = -0.899740  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = -0.899740  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = -0.899740  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
Time spent in simd_matmul_backward weight grads: 0.000011 seconds
matmul simd backward input grads ...
Time spent for input activation grads: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul simd backward weight grads ...
Time spent in simd_matmul_backward weight grads: 0.000017 seconds
matmul simd backward input grads ...
Time spent for input activation grads: 0.000103 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.322806
Training accuracy: 0.062500
matmul simd ....
Time spent in matmul_simd_forward: 0.031509 seconds
relu forward ...
Time spent in relu_forward: 0.000166 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000482 seconds
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
softmax forward ...
Time spent in softmax_forward: 0.000605 seconds
Test loss after training: 2.336064
Test accuracy after training: 0.097200
