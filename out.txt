rows: 28, cols: 28
Number of training images: 60000
rows: 28, cols: 28
Number of test images: 10000
Model created with 2 layers
Layer 0: 784 inputs, 8 neurons
Layer 1: 8 inputs, 10 neurons
Number of parameters: 6370
Batch size: 16

No model files found in directory /home/rickojn/coding/deep/models/

No model found, training from scratch
matmul simd ....
Time spent in matmul_simd_forward: 0.038722 seconds
relu forward ...
Time spent in relu_forward: 0.000149 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000754 seconds
softmax forward ...
Time spent in softmax_forward: 0.000630 seconds
Test loss before training: 2.335670
Test accuracy before training: 0.097500


training loop:

epoch: 0
matmul simd ....
Time spent in matmul_simd_forward: 0.000027 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000002 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul simd backward weight grads ...
wg sum[0][0]: 0.000000  avg = 0.000000
wg sum[0][0]: 0.000000  avg = 0.000000
wg sum[0][0]: 0.027538  avg = 0.001721
wg sum[0][0]: 0.027538  avg = 0.001721
wg sum[0][0]: 0.027538  avg = 0.001721
wg sum[0][0]: 0.027538  avg = 0.001721
wg sum[0][0]: 0.027538  avg = 0.001721
wg sum[0][0]: 0.027538  avg = 0.001721
wg sum[0][0]: 0.122524  avg = 0.007658
wg sum[0][0]: 0.122524  avg = 0.007658
wg sum[0][0]: 0.122524  avg = 0.007658
wg sum[0][0]: 0.122524  avg = 0.007658
wg sum[0][0]: 0.122524  avg = 0.007658
wg sum[0][0]: 0.122524  avg = 0.007658
wg sum[0][0]: 0.122524  avg = 0.007658
wg sum[0][0]: 0.122524  avg = 0.007658
Time spent in simd_matmul_backward weight grads: 0.000007 seconds
matmul simd backward input grads ...
Time spent for input activation grads: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.007658
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul simd backward weight grads ...
Time spent in simd_matmul_backward weight grads: 0.000025 seconds
matmul simd backward input grads ...
Time spent for input activation grads: 0.000053 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.291618
Training accuracy: 0.125000

epoch: 1
matmul simd ....
Time spent in matmul_simd_forward: 0.000025 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000001 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul simd backward weight grads ...
wg sum[0][0]: 0.000000  avg = 0.000000
wg sum[0][0]: 0.000000  avg = 0.000000
wg sum[0][0]: 0.060829  avg = 0.003802
wg sum[0][0]: 0.060829  avg = 0.003802
wg sum[0][0]: 0.060829  avg = 0.003802
wg sum[0][0]: 0.060829  avg = 0.003802
wg sum[0][0]: -0.011247  avg = -0.000703
wg sum[0][0]: -0.011247  avg = -0.000703
wg sum[0][0]: -0.011247  avg = -0.000703
wg sum[0][0]: -0.011247  avg = -0.000703
wg sum[0][0]: -0.011247  avg = -0.000703
wg sum[0][0]: -0.011247  avg = -0.000703
wg sum[0][0]: -0.011247  avg = -0.000703
wg sum[0][0]: -0.011247  avg = -0.000703
wg sum[0][0]: -0.011247  avg = -0.000703
wg sum[0][0]: -0.011247  avg = -0.000703
Time spent in simd_matmul_backward weight grads: 0.000006 seconds
matmul simd backward input grads ...
Time spent for input activation grads: 0.000002 seconds
Layer 1 weight grad [0][0] = -0.000703
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul simd backward weight grads ...
Time spent in simd_matmul_backward weight grads: 0.000025 seconds
matmul simd backward input grads ...
Time spent for input activation grads: 0.000052 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.323008
Training accuracy: 0.062500
matmul simd ....
Time spent in matmul_simd_forward: 0.032642 seconds
relu forward ...
Time spent in relu_forward: 0.000149 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000481 seconds
softmax forward ...
Time spent in softmax_forward: 0.000717 seconds
Test loss after training: 2.335954
Test accuracy after training: 0.097200
