rows: 28, cols: 28
Number of training images: 60000
rows: 28, cols: 28
Number of test images: 10000
Model created with 2 layers
Layer 0: 784 inputs, 8 neurons
Layer 1: 8 inputs, 10 neurons
Number of parameters: 6370
Batch size: 16

No model files found in directory /home/rickojn/coding/deep/models/

No model found, training from scratch
matmul simd ....
Time spent in matmul_simd_forward: 0.033130 seconds
relu forward ...
Time spent in relu_forward: 0.000161 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000750 seconds
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
softmax forward ...
Time spent in softmax_forward: 0.000597 seconds
Test loss before training: 2.335670
Test accuracy before training: 0.097500


training loop:

epoch: 0
matmul simd ....
Time spent in matmul_simd_forward: 0.000029 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000002 seconds
logit = 0.126389
logit = -0.190522
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
logit = 0.000000
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
 input 0.000000   z grad 0.108971  prob 0.108971
 input 0.992157   z grad 0.072773  prob 0.072773
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad -0.900000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad -0.900000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
 input 0.000000   z grad 0.100000  prob 0.100000
matmul backward (separate loops)...
input 0.000000, z grad 0.108971, gradient output sum 0.000000, gradient output average: 0.000000
input 0.992157, z grad 0.072773, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad 0.100000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad 0.100000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad 0.100000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad 0.100000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad 0.100000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad 0.100000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad 0.100000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad -0.900000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad 0.100000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad -0.900000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad 0.100000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad 0.100000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad 0.100000, gradient output sum 0.072202, gradient output average: 0.004513
input 0.000000, z grad 0.100000, gradient output sum 0.072202, gradient output average: 0.004513
Time spent in matmul_backward (weights/biases): 0.000014 seconds
Time spent in matmul_backward (input gradients): 0.000001 seconds
Layer 1 weight grad [0][0] = 0.004513
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward (separate loops)...
Time spent in matmul_backward (weights/biases): 0.000137 seconds
Time spent in matmul_backward (input gradients): 0.000001 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.291618
Training accuracy: 0.125000

epoch: 1
matmul simd ....
Time spent in matmul_simd_forward: 0.000027 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000002 seconds
logit = 0.304777
logit = 0.401730
logit = 0.002614
logit = 0.002614
logit = 0.002614
logit = 0.002614
logit = 0.002614
logit = 0.002614
logit = 0.002614
logit = 0.002614
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
 input 0.000000   z grad 0.118950  prob 0.118950
 input 0.000000   z grad 0.125492  prob 0.125492
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad -0.899740  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad -0.899740  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad -0.899740  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
 input 0.000000   z grad 0.100260  prob 0.100260
matmul backward (separate loops)...
input 0.000000, z grad 0.118950, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad 0.125492, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad 0.100260, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad -0.899740, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad 0.100260, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad -0.899740, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad 0.100260, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad 0.100260, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad 0.100260, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad -0.899740, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad 0.100260, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad 0.100260, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad 0.100260, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad 0.100260, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad 0.100260, gradient output sum 0.000000, gradient output average: 0.000000
input 0.000000, z grad 0.100260, gradient output sum 0.000000, gradient output average: 0.000000
Time spent in matmul_backward (weights/biases): 0.000013 seconds
Time spent in matmul_backward (input gradients): 0.000000 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward (separate loops)...
Time spent in matmul_backward (weights/biases): 0.000136 seconds
Time spent in matmul_backward (input gradients): 0.000001 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.323015
Training accuracy: 0.062500
matmul simd ....
Time spent in matmul_simd_forward: 0.031599 seconds
relu forward ...
Time spent in relu_forward: 0.000181 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000466 seconds
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
logit = 0.011064
softmax forward ...
Time spent in softmax_forward: 0.000639 seconds
Test loss after training: 2.336076
Test accuracy after training: 0.097200
