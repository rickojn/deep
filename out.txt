rows: 28, cols: 28
Number of training images: 60000
rows: 28, cols: 28
Number of test images: 10000
Model created with 2 layers
Layer 0: 784 inputs, 8 neurons
Layer 1: 8 inputs, 10 neurons
Number of parameters: 6370
Batch size: 16

No model files found in directory /home/rickojn/coding/deep/models/

No model found, training from scratch
matmul simd ....
Time spent in matmul_simd_forward: 0.036849 seconds
relu forward ...
Time spent in relu_forward: 0.000170 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000671 seconds
softmax forward ...
Time spent in softmax_forward: 0.000559 seconds
Test loss before training: 2.335670
Test accuracy before training: 0.097500


training loop:

epoch: 0
matmul simd ....
Time spent in matmul_simd_forward: 0.000042 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000002 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
 input 0.000000   z grad 0.108971
 input 0.992157   z grad 0.072773
 input 0.000000   z grad 0.100000
 input 0.000000   z grad 0.100000
 input 0.000000   z grad 0.100000
 input 0.000000   z grad 0.100000
 input 0.000000   z grad 0.100000
 input 0.000000   z grad 0.100000
 input 0.000000   z grad 0.100000
 input 0.000000   z grad -0.900000
 input 0.000000   z grad 0.100000
 input 0.000000   z grad -0.900000
 input 0.000000   z grad 0.100000
 input 0.000000   z grad 0.100000
 input 0.000000   z grad 0.100000
 input 0.000000   z grad 0.100000
matmul simd backward weight grads ...
input = 0.000000 z grad = 0.108971  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.992157 z grad = 0.072773  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = -0.900000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = -0.900000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
input = 0.000000 z grad = 0.100000  wg sum[0][0]: 0.072202  avg = 0.004513
Time spent in simd_matmul_backward weight grads: 0.000030 seconds
matmul simd backward input grads ...
Time spent for input activation grads: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.004513
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul simd backward weight grads ...
Time spent in simd_matmul_backward weight grads: 0.000016 seconds
matmul simd backward input grads ...
Time spent for input activation grads: 0.000087 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.291618
Training accuracy: 0.125000

epoch: 1
matmul simd ....
Time spent in matmul_simd_forward: 0.000028 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000001 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000014 seconds
 input 0.000000   z grad 0.118916
 input 0.000000   z grad 0.125383
 input 0.000000   z grad 0.100260
 input 0.000000   z grad -0.899740
 input 0.000000   z grad 0.100260
 input 0.000000   z grad -0.899740
 input 0.000000   z grad 0.100260
 input 0.000000   z grad 0.100260
 input 0.000000   z grad 0.100260
 input 0.000000   z grad -0.899740
 input 0.000000   z grad 0.100260
 input 0.000000   z grad 0.100260
 input 0.000000   z grad 0.100260
 input 0.000000   z grad 0.100260
 input 0.000000   z grad 0.100260
 input 0.000000   z grad 0.100260
matmul simd backward weight grads ...
input = 0.000000 z grad = 0.118916  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.125383  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = -0.899740  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = -0.899740  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = -0.899740  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
input = 0.000000 z grad = 0.100260  wg sum[0][0]: 0.000000  avg = 0.000000
Time spent in simd_matmul_backward weight grads: 0.000012 seconds
matmul simd backward input grads ...
Time spent for input activation grads: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul simd backward weight grads ...
Time spent in simd_matmul_backward weight grads: 0.000018 seconds
matmul simd backward input grads ...
Time spent for input activation grads: 0.000057 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.322806
Training accuracy: 0.062500
matmul simd ....
Time spent in matmul_simd_forward: 0.034728 seconds
relu forward ...
Time spent in relu_forward: 0.000156 seconds
matmul simd ....
Time spent in matmul_simd_forward: 0.000436 seconds
softmax forward ...
Time spent in softmax_forward: 0.000589 seconds
Test loss after training: 2.336064
Test accuracy after training: 0.097200
